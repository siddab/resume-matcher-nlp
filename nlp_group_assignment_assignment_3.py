# -*- coding: utf-8 -*-
"""NLP_Group_Assignment_Assignment_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10TMLgxRIWlrNb_sq5rnsF4ltw33Aki2m
"""

from google.colab import files
!pip install pdfplumber python-docx spacy
!python -m spacy download en_core_web_sm
import os
import pdfplumber
import docx
import re
import random
import spacy
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

upload_flag = '/content/data/data'
if not os.path.isdir(upload_flag):
  !unzip Resume.zip
  !unzip job_descriptions_cleaned.csv.zip
else:
  print('File already uploaded')

"""The text from the resume pdfs need to be extracted and the extracted text needs to be further cleaned"""

def pdf_to_text(path):
    with pdfplumber.open(path) as pdf:
        return ' '.join([page.extract_text() for page in pdf.pages if page.extract_text()])

def docx_to_text(path):
    doc = docx.Document(path)
    return ' '.join([p.text for p in doc.paragraphs])

def read_text_file(path):
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text

"""The weak verbs and passive voice will be detected through pos tagging and accordingly the function will return the parts of speech with appropriate suggestions."""

weak_verbs = []
with open('Weak_Verbs.txt','r') as file:
  for line in file:
    line = line.strip().lower()
    if line != '':
      weak_verbs.append(line)

def resume_analyser_feedback(resume_doc, weak_verbs=None):
  passive_count = 0
  used_verbs = []
  weak_verbs_found = []
  feedback = []
  passive_verbs_regex = re.compile(r'\b(was|were|is|are|been|being)\s+\w+ed\b')
  for sentence in resume_doc.sents:
    if passive_verbs_regex.search(sentence.text.lower()):
      passive_count = passive_count + 1
    for token in sentence:
      if token.pos_ == "VERB":
        word = token.text.lower()
        used_verbs.append(word)
        if word in weak_verbs:
          if word not in weak_verbs_found:
            weak_verbs_found.append(word)

  if weak_verbs_found:
    feedback.append("Applicant should consider replacing weak verbs like " + ", ".join(f"'{w}'" for w in weak_verbs_found) + " with stronger alternatives")
  else:
    feedback.append("All action words used by the applicant are good")

  return{
      "passive_count": passive_count,
      "verbs": used_verbs,
      "feedback": feedback
  }

"""The resume is scanned to identify the file type and accordingly the POS tags are generated"""

nlp = spacy.load("en_core_web_sm")
resume_details = []
folders = sorted(os.listdir("/content/data/data"))[:24]
extracted_text = ''
base_path = "/content/data/data"
resumes_in_folder = None

for folder in folders:
  folder_path = os.path.join(base_path,folder)
  resumes_in_folder = [f for f in os.listdir(folder_path) if f.lower().endswith(('.pdf', '.docx', '.txt'))]
  print (resumes_in_folder)
  if not resumes_in_folder:
      continue

  selected_file = random.choice(resumes_in_folder)
  full_file_path = os.path.join(folder_path, selected_file)
  file_extension = os.path.splitext(selected_file)[1].lower().strip()

  if file_extension == ".pdf":
    extracted_text = pdf_to_text(full_file_path)
  elif file_extension == ".docx":
    extracted_text = docx_to_text(full_file_path)
  elif file_extension == ".txt":
    extracted_text = read_text_file(full_file_path)
  else:
    print('Unsupported file type. Please upload PDF, DOCX, or TXT.')

  cleaned_text = clean_text(extracted_text)
  user_emails = re.findall(r'\S+@\S+', cleaned_text)
  user_phones = re.findall(r'(\+?\d[\d\-\s]{8,}\d)', cleaned_text)
  document = nlp(cleaned_text)
  analysis = resume_analyser_feedback(document,weak_verbs)
  passive_count = analysis['passive_count']
  verbs = analysis['verbs']
  feedback = analysis['feedback']
  resume_details.append({
      "filename": selected_file,
      "domain": folder,
      "final_text": cleaned_text,
      "pos_tags": [(t.text, t.pos_) for t in document],
      "passive_count": passive_count,
      "verbs" : verbs,
      "feedback": feedback
    })

"""The details and parts of the speech from the resume is then added to a dataframe"""

resume_df = pd.DataFrame(resume_details)
print('The resumes selected are: ')
print(resume_df[['filename','domain']])

jobs_df = pd.read_csv('/content/job_descriptions_cleaned.csv')

"""The code extracts specific columns from the job dataset and accordingly creates a combined string which is then compared with the resume using the sentence BERT"""

chosen_job = jobs_df.sample(1).iloc[0]
jobs_description = chosen_job['Job Description']
important_job_text = " ".join([str(chosen_job['Job Title']), str(chosen_job['Role']), str(chosen_job['skills'])])
print(important_job_text)

model = SentenceTransformer("all-MiniLM-L6-v2")
resume_encoding = model.encode(resume_df['final_text'].tolist(), show_progress_bar=True)
job_text_encoding = model.encode(important_job_text)

"""The cosine similarity is generated for all the selected resumes and based on the value, the resumes and their attributes are sorted"""

similarities = cosine_similarity([job_text_encoding],resume_encoding)[0]
top_ranked_resumes = similarities.argsort()[-24:][::-1]

print(similarities)

top_resume_final = []

for i in top_ranked_resumes:
  row = resume_df.iloc[i]
  top_resume_final.append({
      "file": row['filename'],
      "domain": row['domain'],
      "scores": similarities[i],
      "passive_statements": row['passive_count'],
      "verbs": row['verbs'],
      "feedback": row['feedback']
  })

final_result_data = pd.DataFrame(top_resume_final)
print(final_result_data)

"""A graph has been created to plot the cosine similarity values for all the resumes which belong to different domains"""

plt.figure(figsize=(10,4))
sns.barplot(x='scores',y='domain', data=final_result_data)
plt.title("Resume Matching Score")
plt.xlabel("Score")
plt.tight_layout()
plt.show()

print("POS tags from the top resume: ")
print(resume_df.iloc[top_ranked_resumes[0]]['pos_tags'][:50])

print("Feedback for the top resume: ")
print(resume_df.iloc[top_ranked_resumes[0]]['feedback'])

"""The rankings and properties of the resumes are added to a csv file"""

final_result_data.to_csv("Resume_Rankings_Output.csv", index=False)
files.download('Resume_Rankings_Output.csv')